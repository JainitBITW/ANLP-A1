{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home2/jainit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home2/jainit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:5o0pnpc3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>██████████▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Perplexity</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▁▂▂▁▂▂▁▂▂▁▂▂▁▂▂▁▂</td></tr><tr><td>Train Loss</td><td>█▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂▂▂▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Learning Rate</td><td>0.00011</td></tr><tr><td>Perplexity</td><td>3.18553</td></tr><tr><td>Train Loss</td><td>0.979</td></tr><tr><td>Val Loss</td><td>1.15862</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Cross_entropy</strong> at: <a href='https://wandb.ai/jain_it/LSTM-anlp/runs/5o0pnpc3' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp/runs/5o0pnpc3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230902_132638-5o0pnpc3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:5o0pnpc3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/jainit/ANLP-A1/src/wandb/run-20230902_134437-lttpolma</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jain_it/LSTM-anlp/runs/lttpolma' target=\"_blank\">rose-plasma-25</a></strong> to <a href='https://wandb.ai/jain_it/LSTM-anlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jain_it/LSTM-anlp' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jain_it/LSTM-anlp/runs/lttpolma' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp/runs/lttpolma</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jain_it/LSTM-anlp/runs/lttpolma?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f59640fce20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the libraries \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from torch import cuda\n",
    "import config_hp as hp\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "from  data_maker import *\n",
    "WANDB_SILENT = \"true\"\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project=\"LSTM-anlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, glove_embeddings,hidden_layer, num_layers, dropout):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "        self.vocab_size = glove_embeddings.shape[0]\n",
    "        self.embedding_dim = glove_embeddings.shape[1]\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_embeddings)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=hidden_layer , num_layers=self.num_layers, dropout=dropout, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_layer,self.vocab_size )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "       \n",
    "        embeds = self.embedding(x)\n",
    "        # print(embeds.shape)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "       \n",
    "        return out, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/embeddings.pkl', 'rb') as f:\n",
    "    glove_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_to_id.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/lm_dataloader.pkl','rb') as f:\n",
    "    loaders = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lttpolma) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rose-plasma-25</strong> at: <a href='https://wandb.ai/jain_it/LSTM-anlp/runs/lttpolma' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp/runs/lttpolma</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230902_134437-lttpolma/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lttpolma). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/jainit/ANLP-A1/src/wandb/run-20230902_134446-02kfl2w9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jain_it/LSTM-anlp/runs/02kfl2w9' target=\"_blank\">Cross_entropy_with_ii</a></strong> to <a href='https://wandb.ai/jain_it/LSTM-anlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jain_it/LSTM-anlp' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jain_it/LSTM-anlp/runs/02kfl2w9' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp/runs/02kfl2w9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"LSTM-anlp\", name=\"Cross_entropy_with_ii\",config = {\n",
    "    \"epochs\": 50 ,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": hp.BATCH_SIZE,\n",
    "    \"embeddings_dim\":50,\n",
    "    \"drop_out\":0.5,\n",
    "    \"loss_fn\":\"CEntropy\"\n",
    "\n",
    "})\n",
    "\n",
    "model = LSTM_LM(glove_embeddings, hp.HIDDEN_LAYER, 2, hp.DROPOUT)\n",
    "wandb.watch(model)\n",
    "wandb.config.update({\"model\":model})\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp.LEARNING_RATE)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.8, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 5.923040959372449 ; Perplexity: 373.54592644102246\n",
      "Epoch: 1, Val Loss: 5.416933751264155 ; Perplexity: 225.18758168809623\n",
      "Epoch: 2, Train Loss: 5.239487299278601 ; Perplexity: 188.5733959009218\n",
      "Epoch: 2, Val Loss: 4.934912829999103 ; Perplexity: 139.06102005448506\n",
      "Epoch: 3, Train Loss: 4.901405631352081 ; Perplexity: 134.4786743365455\n",
      "Epoch: 3, Val Loss: 4.705513660481434 ; Perplexity: 110.5550581295374\n",
      "Epoch: 4, Train Loss: 4.719531171103276 ; Perplexity: 112.11567727505823\n",
      "Epoch: 4, Val Loss: 4.570527209351394 ; Perplexity: 96.59502214990032\n",
      "Epoch: 5, Train Loss: 4.597009121990407 ; Perplexity: 99.18721470777889\n",
      "Epoch: 5, Val Loss: 4.486106499930881 ; Perplexity: 88.77512615839568\n",
      "Model Saved at epoch 5\n",
      "Epoch: 6, Train Loss: 4.50332702400842 ; Perplexity: 90.31711921430741\n",
      "Epoch: 6, Val Loss: 4.410634738720016 ; Perplexity: 82.32169969457523\n",
      "Epoch: 7, Train Loss: 4.431610133856345 ; Perplexity: 84.06666658206494\n",
      "Epoch: 7, Val Loss: 4.3645172592819925 ; Perplexity: 78.61144183667676\n",
      "Epoch: 8, Train Loss: 4.370895759891599 ; Perplexity: 79.11446753574127\n",
      "Epoch: 8, Val Loss: 4.323997132825536 ; Perplexity: 75.48976867438486\n",
      "Epoch: 9, Train Loss: 4.319522335585247 ; Perplexity: 75.15272193617082\n",
      "Epoch: 9, Val Loss: 4.294994832664136 ; Perplexity: 73.33183551382137\n",
      "Epoch: 10, Train Loss: 4.276480410398959 ; Perplexity: 71.98663022104904\n",
      "Epoch: 10, Val Loss: 4.273041938314375 ; Perplexity: 71.73953126745378\n",
      "Model Saved at epoch 10\n",
      "Epoch: 11, Train Loss: 4.2356811276376884 ; Perplexity: 69.10873457530667\n",
      "Epoch: 11, Val Loss: 4.2425809269709305 ; Perplexity: 69.58721980380263\n",
      "Epoch: 12, Train Loss: 4.200323987871345 ; Perplexity: 66.70794010371499\n",
      "Epoch: 12, Val Loss: 4.230283329818422 ; Perplexity: 68.73670457318619\n",
      "Epoch: 13, Train Loss: 4.167888572221117 ; Perplexity: 64.57895425748694\n",
      "Epoch: 13, Val Loss: 4.209447431248545 ; Perplexity: 67.3193309721787\n",
      "Epoch: 14, Train Loss: 4.137210955751985 ; Perplexity: 62.627905638633095\n",
      "Epoch: 14, Val Loss: 4.205584263959468 ; Perplexity: 67.05976682751347\n",
      "Epoch: 15, Train Loss: 4.108809789360714 ; Perplexity: 60.87422128942146\n",
      "Epoch: 15, Val Loss: 4.191959311630552 ; Perplexity: 66.15227699294125\n",
      "Model Saved at epoch 15\n",
      "Epoch: 16, Train Loss: 4.084141929520727 ; Perplexity: 59.39095425607301\n",
      "Epoch: 16, Val Loss: 4.196675610068618 ; Perplexity: 66.46500775972642\n",
      "Epoch: 17, Train Loss: 4.061487778417591 ; Perplexity: 58.060628202212015\n",
      "Epoch: 17, Val Loss: 4.185647733953615 ; Perplexity: 65.73606661194806\n",
      "Epoch: 18, Train Loss: 4.040512527229944 ; Perplexity: 56.855475318500325\n",
      "Epoch: 18, Val Loss: 4.179431747916518 ; Perplexity: 65.32871948339162\n",
      "Epoch: 19, Train Loss: 4.018698506772137 ; Perplexity: 55.628658369941256\n",
      "Epoch: 19, Val Loss: 4.182290669308593 ; Perplexity: 65.51575639155402\n",
      "Epoch: 20, Train Loss: 3.9962866636735797 ; Perplexity: 54.39578469707996\n",
      "Epoch: 20, Val Loss: 4.177383184432983 ; Perplexity: 65.19502644032968\n",
      "Model Saved at epoch 20\n",
      "Epoch: 21, Train Loss: 3.9787200153001083 ; Perplexity: 53.44857706390385\n",
      "Epoch: 21, Val Loss: 4.188347980676108 ; Perplexity: 65.91381007797348\n",
      "Epoch: 22, Train Loss: 3.9615576165571396 ; Perplexity: 52.539098016771156\n",
      "Epoch: 22, Val Loss: 4.17944014467151 ; Perplexity: 65.32926803494607\n",
      "Epoch: 23, Train Loss: 3.945693736391535 ; Perplexity: 51.71220030645305\n",
      "Epoch: 23, Val Loss: 4.18350612248806 ; Perplexity: 65.59543613965752\n",
      "Epoch 00023: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch: 24, Train Loss: 3.9198257958711085 ; Perplexity: 50.3916655771959\n",
      "Epoch: 24, Val Loss: 4.190802007321491 ; Perplexity: 66.07576296127232\n",
      "Epoch: 25, Train Loss: 3.9055302229517306 ; Perplexity: 49.67641250455735\n",
      "Epoch: 25, Val Loss: 4.189618300128457 ; Perplexity: 65.9975948785579\n",
      "Model Saved at epoch 25\n",
      "Epoch: 26, Train Loss: 3.893137398050792 ; Perplexity: 49.06458041716399\n",
      "Epoch: 26, Val Loss: 4.187360659340359 ; Perplexity: 65.84876408289631\n",
      "Epoch 00026: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch: 27, Train Loss: 3.874150077417207 ; Perplexity: 48.141764125621265\n",
      "Epoch: 27, Val Loss: 4.201239944293799 ; Perplexity: 66.7690696616134\n",
      "Epoch: 28, Train Loss: 3.8606669592704854 ; Perplexity: 47.4970193844402\n",
      "Epoch: 28, Val Loss: 4.1973805695969535 ; Perplexity: 66.51187941961803\n",
      "Epoch: 29, Train Loss: 3.853506928059592 ; Perplexity: 47.15815383486901\n",
      "Epoch: 29, Val Loss: 4.202334271361496 ; Perplexity: 66.84217685612417\n",
      "Epoch 00029: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch: 30, Train Loss: 3.838141660446297 ; Perplexity: 46.43909459678127\n",
      "Epoch: 30, Val Loss: 4.20430766194072 ; Perplexity: 66.97421281463653\n",
      "Model Saved at epoch 30\n",
      "Epoch: 31, Train Loss: 3.8307423134094107 ; Perplexity: 46.09674376805977\n",
      "Epoch: 31, Val Loss: 4.198368914079982 ; Perplexity: 66.57764856463031\n",
      "Epoch: 32, Train Loss: 3.823065608295042 ; Perplexity: 45.74422747244447\n",
      "Epoch: 32, Val Loss: 4.207390002067515 ; Perplexity: 67.18096860038726\n",
      "Epoch 00032: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch: 33, Train Loss: 3.8104971563383967 ; Perplexity: 45.17289127389842\n",
      "Epoch: 33, Val Loss: 4.204907772556836 ; Perplexity: 67.01441681296913\n",
      "Epoch: 34, Train Loss: 3.806394920166113 ; Perplexity: 44.98796097852063\n",
      "Epoch: 34, Val Loss: 4.223872933166706 ; Perplexity: 68.2974843244251\n",
      "Epoch: 35, Train Loss: 3.8011270642026402 ; Perplexity: 44.75159400019803\n",
      "Epoch: 35, Val Loss: 4.209228589834757 ; Perplexity: 67.3046003265099\n",
      "Epoch 00035: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Model Saved at epoch 35\n",
      "Epoch: 36, Train Loss: 3.789788308682472 ; Perplexity: 44.2470325718657\n",
      "Epoch: 36, Val Loss: 4.2185869690598246 ; Perplexity: 67.93741875891838\n",
      "Epoch: 37, Train Loss: 3.784528914290959 ; Perplexity: 44.01493086913089\n",
      "Epoch: 37, Val Loss: 4.211338728468939 ; Perplexity: 67.44677231246328\n",
      "Epoch: 38, Train Loss: 3.780318858018562 ; Perplexity: 43.83001505945779\n",
      "Epoch: 38, Val Loss: 4.21727834474172 ; Perplexity: 67.84857234658818\n",
      "Epoch 00038: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch: 39, Train Loss: 3.7729273635441305 ; Perplexity: 43.50724010950543\n",
      "Epoch: 39, Val Loss: 4.215377521830678 ; Perplexity: 67.71972672094357\n",
      "Epoch: 40, Train Loss: 3.7685243874978918 ; Perplexity: 43.316099875192\n",
      "Epoch: 40, Val Loss: 4.213308950133671 ; Perplexity: 67.57978839704715\n",
      "Model Saved at epoch 40\n",
      "Epoch: 41, Train Loss: 3.766121485594239 ; Perplexity: 43.21214048846847\n",
      "Epoch: 41, Val Loss: 4.217592225169504 ; Perplexity: 67.86987202810258\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch: 42, Train Loss: 3.7594846265910786 ; Perplexity: 42.92629720483917\n",
      "Epoch: 42, Val Loss: 4.22648534711623 ; Perplexity: 68.47613888341729\n",
      "Epoch: 43, Train Loss: 3.7571179790537497 ; Perplexity: 42.82482591002957\n",
      "Epoch: 43, Val Loss: 4.214302447457976 ; Perplexity: 67.64696209891805\n",
      "Epoch: 44, Train Loss: 3.7544708760308305 ; Perplexity: 42.71161409168814\n",
      "Epoch: 44, Val Loss: 4.235697550489413 ; Perplexity: 69.1098695471272\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch: 45, Train Loss: 3.7492062633734013 ; Perplexity: 42.48734485085348\n",
      "Epoch: 45, Val Loss: 4.232963154647524 ; Perplexity: 68.92115393637899\n",
      "Model Saved at epoch 45\n",
      "Epoch: 46, Train Loss: 3.7459968254764453 ; Perplexity: 42.35120294250881\n",
      "Epoch: 46, Val Loss: 4.242186536852097 ; Perplexity: 69.55978070312568\n",
      "Epoch: 47, Train Loss: 3.7413660122641623 ; Perplexity: 42.15553583095329\n",
      "Epoch: 47, Val Loss: 4.235864285601686 ; Perplexity: 69.12139354968652\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch: 48, Train Loss: 3.7403148303408105 ; Perplexity: 42.11124597614426\n",
      "Epoch: 48, Val Loss: 4.232456251485458 ; Perplexity: 68.88622643869282\n",
      "Epoch: 49, Train Loss: 3.737426233189955 ; Perplexity: 41.98977906993811\n",
      "Epoch: 49, Val Loss: 4.239320298693827 ; Perplexity: 69.36069126057069\n",
      "Epoch: 50, Train Loss: 3.7370446470500562 ; Perplexity: 41.9737594088623\n",
      "Epoch: 50, Val Loss: 4.237716205862184 ; Perplexity: 69.24951946170276\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Model Saved at epoch 50\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(hp.EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(loaders['train']):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, hidden = model(x)\n",
    "        loss = criterion(out.view(-1, model.vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss/len(loaders[\"train\"])} ; Perplexity: {np.exp(train_loss/len(loaders[\"train\"]))}')\n",
    "    wandb.log({\"Train Loss\": train_loss/len(loaders[\"train\"])})\n",
    "    wandb.log({\"Perplexity\": np.exp(train_loss/len(loaders[\"train\"]))})\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loaders['val']):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out, hidden = model(x)\n",
    "            loss = criterion(out.view(-1, model.vocab_size), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "        print(f'Epoch: {epoch+1}, Val Loss: {val_loss/len(loaders[\"val\"])} ; Perplexity: {np.exp(val_loss/len(loaders[\"val\"]))}')\n",
    "        wandb.log({\"Val Loss\": val_loss/len(loaders[\"val\"])})\n",
    "        wandb.log({\"Perplexity\": np.exp(val_loss/len(loaders[\"val\"]))})\n",
    "        scheduler.step(val_loss/len(loaders[\"val\"]))\n",
    "        wandb.log({\"Learning Rate\": optimizer.param_groups[0]['lr']})\n",
    "        if (epoch+1)%5==0:\n",
    "            torch.save(model.state_dict(), f'../data/models/lstm_{epoch+1}.pt')\n",
    "            print(f'Model Saved at epoch {epoch+1}')\n",
    "           \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
