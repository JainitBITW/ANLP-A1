{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home2/jainit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home2/jainit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home2/jainit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjain_it\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_38244/1585703692.py 22 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m lr_scheduler\n\u001b[1;32m     20\u001b[0m wandb\u001b[39m.\u001b[39mlogin()\n\u001b[0;32m---> 22\u001b[0m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLSTM-anlp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anlp/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1187\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[1;32m   1186\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[0;32m-> 1187\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1188\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1189\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/anlp/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:1164\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1162\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1163\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1164\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1165\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1166\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anlp/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:750\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    747\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcommunicating run to backend with \u001b[39m\u001b[39m{\u001b[39;00mtimeout\u001b[39m}\u001b[39;00m\u001b[39m second timeout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m run_init_handle \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_run(run)\n\u001b[0;32m--> 750\u001b[0m result \u001b[39m=\u001b[39m run_init_handle\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    751\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    752\u001b[0m     on_progress\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_progress_init,\n\u001b[1;32m    753\u001b[0m     cancel\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    754\u001b[0m )\n\u001b[1;32m    755\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    756\u001b[0m     run_result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mrun_result\n",
      "File \u001b[0;32m~/anlp/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot\u001b[39m.\u001b[39;49m_get_and_clear(timeout\u001b[39m=\u001b[39;49mwait_timeout)\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m on_progress \u001b[39mand\u001b[39;00m progress_handle \u001b[39mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m~/anlp/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_and_clear\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[pb\u001b[39m.\u001b[39mResult], \u001b[39mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout):\n\u001b[1;32m    131\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anlp/lib/python3.9/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m~/local_py/py3.9/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/local_py/py3.9/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# importing the libraries \n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from torch import cuda\n",
    "import config_hp as hp\n",
    "from pprint import pprint\n",
    "import pickle \n",
    "from  data_maker import *\n",
    "WANDB_SILENT = \"true\"\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project=\"LSTM-anlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, glove_embeddings,hidden_layer, num_layers, dropout):\n",
    "        super(LSTM_LM, self).__init__()\n",
    "        self.vocab_size = glove_embeddings.shape[0]\n",
    "        self.embedding_dim = glove_embeddings.shape[1]\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove_embeddings)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=hidden_layer , num_layers=self.num_layers, dropout=dropout, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_layer,self.vocab_size )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "       \n",
    "        embeds = self.embedding(x) # batch_size, seq_len, embedding_dim\n",
    "        # print(embeds.shape)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds) # lstm output shape: batch_size, seq_len, hidden_layer\n",
    "        lstm_out = self.dropout(lstm_out) # batch_size, seq_len, hidden_layer\n",
    "        out = self.fc(lstm_out) # batch_size, seq_len, vocab_size\n",
    "       \n",
    "        return out, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/embeddings.pkl', 'rb') as f:\n",
    "    glove_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/word_to_id.json', 'r') as f:\n",
    "    word2idx = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/lm_dataloader.pkl','rb') as f:\n",
    "    loaders = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:8l1qwi7m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-frost-27</strong> at: <a href='https://wandb.ai/jain_it/LSTM-anlp/runs/8l1qwi7m' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp/runs/8l1qwi7m</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230902_162956-8l1qwi7m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:8l1qwi7m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home2/jainit/ANLP-A1/src/wandb/run-20230902_162959-nmdpx3n8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jain_it/LSTM-anlp/runs/nmdpx3n8' target=\"_blank\">Cross_entropy_with__300_dim</a></strong> to <a href='https://wandb.ai/jain_it/LSTM-anlp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jain_it/LSTM-anlp' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jain_it/LSTM-anlp/runs/nmdpx3n8' target=\"_blank\">https://wandb.ai/jain_it/LSTM-anlp/runs/nmdpx3n8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"LSTM-anlp\", name=\"Cross_entropy_with__300_dim\",config = {\n",
    "    \"epochs\": 50 ,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"batch_size\": hp.BATCH_SIZE,\n",
    "    \"embeddings_dim\":300,\n",
    "    \"drop_out\":hp.DROPOUT,\n",
    "    \"loss_fn\":\"CEntropy\"\n",
    "\n",
    "})\n",
    "\n",
    "model = LSTM_LM(glove_embeddings, hp.HIDDEN_LAYER, 2, hp.DROPOUT)\n",
    "wandb.watch(model)\n",
    "wandb.config.update({\"model\":model})\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp.LEARNING_RATE)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.8, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 5.714415735273219 ; Perplexity: 303.20699839308315\n",
      "Epoch: 1, Val Loss: 5.064549768207878 ; Perplexity: 158.309150213545\n",
      "Epoch: 2, Train Loss: 4.922901514496631 ; Perplexity: 137.40070551074294\n",
      "Epoch: 2, Val Loss: 4.6711088963691765 ; Perplexity: 106.81612479638952\n",
      "Epoch: 3, Train Loss: 4.661872010749541 ; Perplexity: 105.83401925037508\n",
      "Epoch: 3, Val Loss: 4.49945464986839 ; Perplexity: 89.9680538295389\n",
      "Epoch: 4, Train Loss: 4.515827855575822 ; Perplexity: 91.4532447711398\n",
      "Epoch: 4, Val Loss: 4.385991582807327 ; Perplexity: 80.31782551514425\n",
      "Epoch: 5, Train Loss: 4.418798117241117 ; Perplexity: 82.99647335882008\n",
      "Epoch: 5, Val Loss: 4.324961087561601 ; Perplexity: 75.56257247857141\n",
      "Model Saved at epoch 5\n",
      "Epoch: 6, Train Loss: 4.341956690684564 ; Perplexity: 76.85777920449351\n",
      "Epoch: 6, Val Loss: 4.266074499547087 ; Perplexity: 71.24142774409636\n",
      "Epoch: 7, Train Loss: 4.277046339344114 ; Perplexity: 72.02738106870983\n",
      "Epoch: 7, Val Loss: 4.228770181832724 ; Perplexity: 68.63277441780038\n",
      "Epoch: 8, Train Loss: 4.220610275451563 ; Perplexity: 68.07501612640706\n",
      "Epoch: 8, Val Loss: 4.184485566537112 ; Perplexity: 65.65971467270421\n",
      "Epoch: 9, Train Loss: 4.174099608016675 ; Perplexity: 64.98130466696371\n",
      "Epoch: 9, Val Loss: 4.154708532308111 ; Perplexity: 63.73338561756325\n",
      "Epoch: 10, Train Loss: 4.131506483183741 ; Perplexity: 62.27166352164222\n",
      "Epoch: 10, Val Loss: 4.137453293958247 ; Perplexity: 62.643084612095834\n",
      "Model Saved at epoch 10\n",
      "Epoch: 11, Train Loss: 4.092658388080881 ; Perplexity: 59.89891479910305\n",
      "Epoch: 11, Val Loss: 4.12288246249521 ; Perplexity: 61.73694045263566\n",
      "Epoch: 12, Train Loss: 4.058643026900953 ; Perplexity: 57.895694850506416\n",
      "Epoch: 12, Val Loss: 4.103970347650793 ; Perplexity: 60.58033573842243\n",
      "Epoch: 13, Train Loss: 4.027589112710851 ; Perplexity: 56.12543590483605\n",
      "Epoch: 13, Val Loss: 4.100002219345396 ; Perplexity: 60.340421513450046\n",
      "Epoch: 14, Train Loss: 3.9976319910875007 ; Perplexity: 54.46901408514146\n",
      "Epoch: 14, Val Loss: 4.092477163731657 ; Perplexity: 59.88806064079795\n",
      "Epoch: 15, Train Loss: 3.9714525833821246 ; Perplexity: 53.061551214399\n",
      "Epoch: 15, Val Loss: 4.087919159440805 ; Perplexity: 59.615711758626134\n",
      "Model Saved at epoch 15\n",
      "Epoch: 16, Train Loss: 3.947654551280333 ; Perplexity: 51.813697835159395\n",
      "Epoch: 16, Val Loss: 4.095784567839263 ; Perplexity: 60.086462575385994\n",
      "Epoch: 17, Train Loss: 3.924001820814381 ; Perplexity: 50.60254243663185\n",
      "Epoch: 17, Val Loss: 4.085139427753473 ; Perplexity: 59.45022618489205\n",
      "Epoch: 18, Train Loss: 3.900929393036279 ; Perplexity: 49.44838474052714\n",
      "Epoch: 18, Val Loss: 4.077804071224288 ; Perplexity: 59.015733109140704\n",
      "Epoch: 19, Train Loss: 3.8789416589716605 ; Perplexity: 48.372992847841495\n",
      "Epoch: 19, Val Loss: 4.080814410519126 ; Perplexity: 59.19365816255759\n",
      "Epoch: 20, Train Loss: 3.85999715912825 ; Perplexity: 47.465216526069256\n",
      "Epoch: 20, Val Loss: 4.075755258269657 ; Perplexity: 58.894944689311444\n",
      "Model Saved at epoch 20\n",
      "Epoch: 21, Train Loss: 3.8438225880360553 ; Perplexity: 46.70366251457968\n",
      "Epoch: 21, Val Loss: 4.080765406817 ; Perplexity: 59.19075752523678\n",
      "Epoch: 22, Train Loss: 3.825827520793435 ; Perplexity: 45.87074365892448\n",
      "Epoch: 22, Val Loss: 4.078349516091757 ; Perplexity: 59.04793171834978\n",
      "Epoch: 23, Train Loss: 3.808865009340396 ; Perplexity: 45.09922261041254\n",
      "Epoch: 23, Val Loss: 4.08500798016984 ; Perplexity: 59.44241210989545\n",
      "Epoch 00023: reducing learning rate of group 0 to 8.0000e-04.\n",
      "Epoch: 24, Train Loss: 3.7841177800062624 ; Perplexity: 43.996838541455\n",
      "Epoch: 24, Val Loss: 4.087012620951167 ; Perplexity: 59.56169231038375\n",
      "Epoch: 25, Train Loss: 3.7676411157986247 ; Perplexity: 43.27785688201274\n",
      "Epoch: 25, Val Loss: 4.090550803190825 ; Perplexity: 59.772805690847726\n",
      "Model Saved at epoch 25\n",
      "Epoch: 26, Train Loss: 3.7559761431679797 ; Perplexity: 42.77595489365357\n",
      "Epoch: 26, Val Loss: 4.0969129256065315 ; Perplexity: 60.15429986731077\n",
      "Epoch 00026: reducing learning rate of group 0 to 6.4000e-04.\n",
      "Epoch: 27, Train Loss: 3.736524172174905 ; Perplexity: 41.95191880590994\n",
      "Epoch: 27, Val Loss: 4.093409650373143 ; Perplexity: 59.94393150269773\n",
      "Epoch: 28, Train Loss: 3.725969763452819 ; Perplexity: 41.511469537622226\n",
      "Epoch: 28, Val Loss: 4.096576780672894 ; Perplexity: 60.13408270231303\n",
      "Epoch: 29, Train Loss: 3.713846344429293 ; Perplexity: 41.01124692904494\n",
      "Epoch: 29, Val Loss: 4.101833158770934 ; Perplexity: 60.451002372688016\n",
      "Epoch 00029: reducing learning rate of group 0 to 5.1200e-04.\n",
      "Epoch: 30, Train Loss: 3.699591804923279 ; Perplexity: 40.430797338832576\n",
      "Epoch: 30, Val Loss: 4.103183567918689 ; Perplexity: 60.53269110347373\n",
      "Model Saved at epoch 30\n",
      "Epoch: 31, Train Loss: 3.689248907540653 ; Perplexity: 40.014780867321576\n",
      "Epoch: 31, Val Loss: 4.104379390249189 ; Perplexity: 60.605120745080775\n",
      "Epoch: 32, Train Loss: 3.6802496335653863 ; Perplexity: 39.656292378702965\n",
      "Epoch: 32, Val Loss: 4.118044278479569 ; Perplexity: 61.43896718017204\n",
      "Epoch 00032: reducing learning rate of group 0 to 4.0960e-04.\n",
      "Epoch: 33, Train Loss: 3.6690977351751917 ; Perplexity: 39.216506218667305\n",
      "Epoch: 33, Val Loss: 4.119332056171847 ; Perplexity: 61.51813787773112\n",
      "Epoch: 34, Train Loss: 3.661219367086252 ; Perplexity: 38.90875801611583\n",
      "Epoch: 34, Val Loss: 4.12428745370827 ; Perplexity: 61.82374127441002\n",
      "Epoch: 35, Train Loss: 3.6566899732740197 ; Perplexity: 38.73292344094411\n",
      "Epoch: 35, Val Loss: 4.125392760662054 ; Perplexity: 61.89211326460682\n",
      "Epoch 00035: reducing learning rate of group 0 to 3.2768e-04.\n",
      "Model Saved at epoch 35\n",
      "Epoch: 36, Train Loss: 3.6448557722543096 ; Perplexity: 38.27725181168395\n",
      "Epoch: 36, Val Loss: 4.114406969373589 ; Perplexity: 61.21590059238227\n",
      "Epoch: 37, Train Loss: 3.6393234983944436 ; Perplexity: 38.066076251179226\n",
      "Epoch: 37, Val Loss: 4.137903750337513 ; Perplexity: 62.671308945614534\n",
      "Epoch: 38, Train Loss: 3.6363529909902543 ; Perplexity: 37.95316846955136\n",
      "Epoch: 38, Val Loss: 4.129490991301884 ; Perplexity: 62.14628188462424\n",
      "Epoch 00038: reducing learning rate of group 0 to 2.6214e-04.\n",
      "Epoch: 39, Train Loss: 3.6280264102065485 ; Perplexity: 37.63846038570867\n",
      "Epoch: 39, Val Loss: 4.140644041907708 ; Perplexity: 62.84328212592444\n",
      "Epoch: 40, Train Loss: 3.623664660240287 ; Perplexity: 37.4746483456796\n",
      "Epoch: 40, Val Loss: 4.143738939272647 ; Perplexity: 63.03807691363265\n",
      "Model Saved at epoch 40\n",
      "Epoch: 41, Train Loss: 3.6190550571311513 ; Perplexity: 37.30230261802566\n",
      "Epoch: 41, Val Loss: 4.144170186377519 ; Perplexity: 63.065267764364755\n",
      "Epoch 00041: reducing learning rate of group 0 to 2.0972e-04.\n",
      "Epoch: 42, Train Loss: 3.6123366655825553 ; Perplexity: 37.0525311142431\n",
      "Epoch: 42, Val Loss: 4.141363303392928 ; Perplexity: 62.8884991378406\n",
      "Epoch: 43, Train Loss: 3.6092393225444153 ; Perplexity: 36.937944263950165\n",
      "Epoch: 43, Val Loss: 4.148454295088913 ; Perplexity: 63.33602579167729\n",
      "Epoch: 44, Train Loss: 3.605724759701727 ; Perplexity: 36.80835140207916\n",
      "Epoch: 44, Val Loss: 4.148400631961444 ; Perplexity: 63.33262707364554\n",
      "Epoch 00044: reducing learning rate of group 0 to 1.6777e-04.\n",
      "Epoch: 45, Train Loss: 3.5994118729125715 ; Perplexity: 36.576716358954094\n",
      "Epoch: 45, Val Loss: 4.15518619366829 ; Perplexity: 63.763835865100525\n",
      "Model Saved at epoch 45\n",
      "Epoch: 46, Train Loss: 3.5981722803258185 ; Perplexity: 36.5314042226041\n",
      "Epoch: 46, Val Loss: 4.147963760704394 ; Perplexity: 63.30496491208525\n",
      "Epoch: 47, Train Loss: 3.5939430204281675 ; Perplexity: 36.37722967164868\n",
      "Epoch: 47, Val Loss: 4.150275391458676 ; Perplexity: 63.451471885647635\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.3422e-04.\n",
      "Epoch: 48, Train Loss: 3.591200638427409 ; Perplexity: 36.27760607717211\n",
      "Epoch: 48, Val Loss: 4.14036948949296 ; Perplexity: 62.82603071938206\n",
      "Epoch: 49, Train Loss: 3.588496964623425 ; Perplexity: 36.17965573650195\n",
      "Epoch: 49, Val Loss: 4.157448174937671 ; Perplexity: 63.90823171627916\n",
      "Epoch: 50, Train Loss: 3.5853112130276936 ; Perplexity: 36.064579739602394\n",
      "Epoch: 50, Val Loss: 4.161431213088383 ; Perplexity: 64.16328825411276\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0737e-04.\n",
      "Model Saved at epoch 50\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(hp.EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(loaders['train']):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, hidden = model(x)\n",
    "        loss = criterion(out.view(-1, model.vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss/len(loaders[\"train\"])} ; Perplexity: {np.exp(train_loss/len(loaders[\"train\"]))}')\n",
    "    wandb.log({\"Train Loss\": train_loss/len(loaders[\"train\"])})\n",
    "    wandb.log({\"Perplexity\": np.exp(train_loss/len(loaders[\"train\"]))})\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loaders['val']):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out, hidden = model(x)\n",
    "            loss = criterion(out.view(-1, model.vocab_size), y.view(-1))\n",
    "            val_loss += loss.item()\n",
    "        print(f'Epoch: {epoch+1}, Val Loss: {val_loss/len(loaders[\"val\"])} ; Perplexity: {np.exp(val_loss/len(loaders[\"val\"]))}')\n",
    "        wandb.log({\"Val Loss\": val_loss/len(loaders[\"val\"])})\n",
    "        wandb.log({\"Perplexity\": np.exp(val_loss/len(loaders[\"val\"]))})\n",
    "        scheduler.step(val_loss/len(loaders[\"val\"]))\n",
    "        wandb.log({\"Learning Rate\": optimizer.param_groups[0]['lr']})\n",
    "        if (epoch+1)%5==0:\n",
    "            torch.save(model.state_dict(), f'../data/models/lstm_{epoch+1}.pt')\n",
    "            print(f'Model Saved at epoch {epoch+1}')\n",
    "           \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
