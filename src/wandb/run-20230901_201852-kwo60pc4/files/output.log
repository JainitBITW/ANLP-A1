
The Device is cuda
/home2/jainit/anlp/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch: 1, Loss: 5.51192086220739
/home2/jainit/anlp/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:389: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
VALID: Epoch: 1, Loss: 5.036436464135612
Epoch: 2, Loss: 4.897023087146054
VALID: Epoch: 2, Loss: 4.81425431178804
Epoch: 3, Loss: 4.641587883951302
VALID: Epoch: 3, Loss: 4.798608895772439
Epoch: 4, Loss: 4.514967046900125
VALID: Epoch: 4, Loss: 4.828179265707672
Epoch: 5, Loss: 4.436832130780346
VALID: Epoch: 5, Loss: 4.917850150180306
Epoch: 6, Loss: 4.3869718520893155
VALID: Epoch: 6, Loss: 4.983342483174607
Epoch: 7, Loss: 4.35275893640096
VALID: Epoch: 7, Loss: 5.048169014812787
Epoch: 8, Loss: 4.329113085707876
VALID: Epoch: 8, Loss: 5.123949269445834
Epoch: 9, Loss: 4.312309092187364
VALID: Epoch: 9, Loss: 5.172716775650754
[nltk_data] Downloading package punkt to /home2/jainit/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data]     /home2/jainit/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.